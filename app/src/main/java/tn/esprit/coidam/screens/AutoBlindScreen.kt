package tn.esprit.coidam.screens

import android.Manifest
import android.graphics.Bitmap
import android.graphics.BitmapFactory
import android.graphics.ImageFormat
import android.graphics.Rect
import android.graphics.YuvImage
import android.util.Log
import androidx.camera.core.CameraSelector
import androidx.camera.core.ImageCapture
import androidx.camera.core.ImageCaptureException
import androidx.camera.core.ImageAnalysis
import androidx.camera.core.ImageProxy
import androidx.camera.core.Preview
import androidx.camera.lifecycle.ProcessCameraProvider
import androidx.camera.view.PreviewView
import androidx.compose.foundation.background
import androidx.compose.foundation.layout.*
import androidx.compose.foundation.shape.CircleShape
import androidx.compose.foundation.shape.RoundedCornerShape
import androidx.compose.material.icons.Icons
import androidx.compose.material.icons.filled.*
import androidx.compose.material3.*
import androidx.compose.runtime.*
import androidx.compose.ui.Alignment
import androidx.compose.ui.Modifier
import androidx.compose.ui.graphics.Color
import androidx.compose.ui.platform.LocalContext
import androidx.compose.ui.text.font.FontWeight
import androidx.compose.ui.unit.dp
import androidx.compose.ui.unit.sp
import androidx.compose.ui.viewinterop.AndroidView
import androidx.core.content.ContextCompat
import androidx.lifecycle.compose.LocalLifecycleOwner
import androidx.navigation.NavController
import com.google.accompanist.permissions.ExperimentalPermissionsApi
import com.google.accompanist.permissions.rememberMultiplePermissionsState
import kotlinx.coroutines.delay
import kotlinx.coroutines.launch
import tn.esprit.coidam.data.api.VoiceCommandService
import tn.esprit.coidam.data.api.VoiceWebSocketClient
import tn.esprit.coidam.data.local.TokenManager
import tn.esprit.coidam.data.models.Enums.ConnectionState
import tn.esprit.coidam.data.models.Enums.DetectionState
import tn.esprit.coidam.data.repository.FaceRecognitionRepository
import com.google.mlkit.vision.common.InputImage
import com.google.mlkit.vision.face.FaceDetection
import com.google.mlkit.vision.face.FaceDetectorOptions
import java.io.ByteArrayOutputStream
import kotlin.coroutines.resume
import kotlin.coroutines.suspendCoroutine

@OptIn(ExperimentalPermissionsApi::class)
@Composable
fun AutoBlindScreen(navController: NavController) {
    val context = LocalContext.current
    val scope = rememberCoroutineScope()
    val tokenManager = remember { TokenManager(context) }

    // ‚úÖ Services (using singleton)
    val voiceService = remember { VoiceCommandService(context) }
    val wsClient = remember { VoiceWebSocketClient.getInstance(context) }
    val faceRepository = remember { FaceRecognitionRepository(context) }

    // ‚úÖ √âtats WebSocket et Voice
    val isVoiceReady by voiceService.isReady.collectAsState()
    val recognizedText by voiceService.recognizedText.collectAsState()
    val connectionState by wsClient.connectionState.collectAsState()
    val voiceInstruction by wsClient.voiceInstruction.collectAsState()
    val voiceResponse by wsClient.voiceResponse.collectAsState()

    // ‚úÖ √âtats UI
    var isListening by remember { mutableStateOf(false) }
    var isCameraActive by remember { mutableStateOf(false) }
    var showExitDialog by remember { mutableStateOf(false) }
    var currentBitmap by remember { mutableStateOf<Bitmap?>(null) }
    
    // ‚úÖ OPTIMISATION: Contr√¥le de capture pour √©viter les captures multiples
    var alreadyCaptured by remember { mutableStateOf(false) }
    var cameraAnalysisEnabled by remember { mutableStateOf(true) }
    
    // ‚úÖ MACHINE √Ä √âTATS - Variables d'√©tat
    var detectionState by remember { mutableStateOf(DetectionState.IDLE) }
    var faceFirstDetectedAt by remember { mutableLongStateOf(0L) }
    var faceLastSeenAt by remember { mutableLongStateOf(0L) }
    var lastRecognitionAt by remember { mutableLongStateOf(0L) }
    var lastAnalysisTime by remember { mutableLongStateOf(0L) }
    var currentPersonId by remember { mutableStateOf<String?>(null) }
    var currentPersonName by remember { mutableStateOf<String?>(null) }
    var hasAnnouncedDetection by remember { mutableStateOf(false) }
    var hasAnnouncedPerson by remember { mutableStateOf(false) }
    var recognitionAttempts by remember { mutableIntStateOf(0) }
    var consecutiveramesWithFace by remember { mutableIntStateOf(0) }
    var consecutiveFramesWithoutFace by remember { mutableIntStateOf(0) }
    var detectionCount by remember { mutableIntStateOf(0) }
    var lastFacePosition by remember { mutableStateOf<android.graphics.RectF?>(null) }  // ‚úÖ Tracker position du visage
    var isProcessingCommand by remember { mutableStateOf(false) } // ‚úÖ √âtat occup√© pour √©viter conflits
    
    // ‚úÖ Configuration (ULTRA-OPTIMIS√â pour r√©activit√© maximale - utilisateur aveugle)
    val STABILITY_THRESHOLD_MS = 300L              // ‚úÖ R√©duit √† 300ms pour d√©clenchement ultra-rapide
    val DISAPPEAR_THRESHOLD_MS = 2000L             // ‚úÖ R√©duit √† 2s sans visage = reset rapide (pour changement de direction)
    val RECOGNITION_COOLDOWN_MS = 2000L            // ‚úÖ Augment√© √† 2s pour √©viter rafales
    val MIN_CONFIDENCE = 0.3f                      // Confiance minimum pour annoncer
    val MAX_RECOGNITION_ATTEMPTS = 3               // Maximum 3 captures
    val FRAME_ANALYSIS_INTERVAL_MS = 200L          // ‚úÖ R√©duit √† 200ms entre analyses (plus r√©actif)
    val MAX_FRAMES_WITHOUT_FACE_BEFORE_RESET = 10  // ‚úÖ R√©duit √† 10 frames (~2s) pour reset rapide
    
    // ‚úÖ ML Kit Face Detector - OPTIMIS√â POUR PERFORMANCE MAXIMALE
    val faceDetector = remember {
        val options = FaceDetectorOptions.Builder()
            .setPerformanceMode(FaceDetectorOptions.PERFORMANCE_MODE_FAST)  // ‚úÖ Mode rapide
            .setMinFaceSize(0.15f)
            .setLandmarkMode(FaceDetectorOptions.LANDMARK_MODE_NONE)        // ‚úÖ D√©sactiv√© (non n√©cessaire)
            .setClassificationMode(FaceDetectorOptions.CLASSIFICATION_MODE_NONE)  // ‚úÖ D√©sactiv√© (non n√©cessaire)
            .build()
        FaceDetection.getClient(options)
    }
    
    // ‚úÖ D√©tection locale de visage (ML Kit) avec tracking de position
    suspend fun detectFaceLocally(bitmap: Bitmap): Pair<Boolean, android.graphics.RectF?> {
        return suspendCoroutine { continuation ->
            val image = InputImage.fromBitmap(bitmap, 0)
            faceDetector.process(image)
                .addOnSuccessListener { faces ->
                    if (faces.isNotEmpty()) {
                        // Retourner le bounding box du premier visage
                        val face = faces[0]
                        val bounds = face.boundingBox
                        val rectF = android.graphics.RectF(
                            bounds.left.toFloat(),
                            bounds.top.toFloat(),
                            bounds.right.toFloat(),
                            bounds.bottom.toFloat()
                        )
                        continuation.resume(Pair(true, rectF))
                    } else {
                        continuation.resume(Pair(false, null))
                    }
                }
                .addOnFailureListener { e ->
                    Log.e("AutoBlind", "Face detection failed: ${e.message}")
                    continuation.resume(Pair(false, null))
                }
        }
    }
    
    // ‚úÖ V√©rifier si le visage a significativement chang√© de position (plus sensible pour changement de direction)
    fun hasFacePositionChanged(oldRect: android.graphics.RectF?, newRect: android.graphics.RectF?): Boolean {
        if (oldRect == null || newRect == null) return true
        
        // Calculer le d√©placement du centre
        val oldCenterX = (oldRect.left + oldRect.right) / 2
        val oldCenterY = (oldRect.top + oldRect.bottom) / 2
        val newCenterX = (newRect.left + newRect.right) / 2
        val newCenterY = (newRect.top + newRect.bottom) / 2
        
        val deltaX = kotlin.math.abs(newCenterX - oldCenterX)
        val deltaY = kotlin.math.abs(newCenterY - oldCenterY)
        
        // ‚úÖ Utiliser la largeur du visage comme r√©f√©rence (40% de d√©placement - moins sensible pour √©viter rafales)
        val faceWidth = oldRect.right - oldRect.left
        val threshold = faceWidth * 0.4f
        
        val changed = deltaX > threshold || deltaY > threshold
        if (changed) {
            Log.d("AutoBlind", "üìç Face moved significantly: deltaX=$deltaX, deltaY=$deltaY, threshold=$threshold")
        }
        
        return changed
    }
    
    // ‚úÖ Reset vers IDLE
    fun resetToIdle() {
        Log.d("AutoBlind", "üîÑ Resetting to IDLE state")
        detectionState = DetectionState.IDLE
        faceFirstDetectedAt = 0L
        faceLastSeenAt = 0L
        consecutiveramesWithFace = 0
        consecutiveFramesWithoutFace = 0
        hasAnnouncedDetection = false
        hasAnnouncedPerson = false
        recognitionAttempts = 0
        currentPersonId = null
        currentPersonName = null
        lastFacePosition = null  // ‚úÖ Reset position
        alreadyCaptured = false  // ‚úÖ Reset capture flag
        cameraAnalysisEnabled = true  // ‚úÖ Re-enable analysis
    }
    
    // ‚úÖ Annonce personne reconnue
    fun announceRecognizedPerson(person: tn.esprit.coidam.data.models.FaceRecognition.RecognizedPerson) {
        val confidencePercent = (person.confidence * 100).toInt()
        val announcement = buildString {
            append("C'est ${person.name}")
            person.relation?.let { append(", votre $it") }
            append(". Confiance $confidencePercent pour cent.")
        }
        Log.d("AutoBlind", "üó£Ô∏è Announcing: $announcement")
        voiceService.speak(announcement)
    }
    
    // ‚úÖ Annonce personne inconnue
    fun announceUnknownPerson() {
        val announcement = "Personne inconnue devant vous."
        Log.d("AutoBlind", "üó£Ô∏è Announcing: $announcement")
        voiceService.speak(announcement)
    }
    
    // ‚úÖ Reconnaissance faciale
    suspend fun startRecognition(bitmap: Bitmap) {
        val currentTime = System.currentTimeMillis()
        
        // ‚úÖ Eviter les chevauchements : Si d√©j√† en reconnaissance, annuler
        if (detectionState == DetectionState.RECOGNIZING) {
             Log.d("AutoBlind", "‚ö†Ô∏è Recognition already in progress, skipping request")
             return
        }
        
        // ‚úÖ V√©rifier cooldown (mais permettre si visage a chang√© de position)
        val timeSinceLastRecognition = currentTime - lastRecognitionAt
        if (timeSinceLastRecognition < RECOGNITION_COOLDOWN_MS) {
            // ‚úÖ Si le visage a chang√© de position, ignorer le cooldown (changement de direction)
            if (lastFacePosition != null) {
                val (hasFace, facePosition) = detectFaceLocally(bitmap)
                if (hasFace && hasFacePositionChanged(lastFacePosition, facePosition)) {
                    Log.d("AutoBlind", "üîÑ Face position changed, ignoring cooldown for new detection")
                    // R√©initialiser les tentatives pour nouvelle personne
                    recognitionAttempts = 0
                    hasAnnouncedPerson = false
                    currentPersonId = null
                    currentPersonName = null
                } else {
                    Log.d("AutoBlind", "‚è≥ Recognition cooldown active (${timeSinceLastRecognition}ms < ${RECOGNITION_COOLDOWN_MS}ms), skipping...")
                    return
                }
            } else {
                Log.d("AutoBlind", "‚è≥ Recognition cooldown active, skipping...")
                return
            }
        }
        
        // V√©rifier nombre de tentatives
        if (recognitionAttempts >= MAX_RECOGNITION_ATTEMPTS) {
            Log.w("AutoBlind", "‚ö†Ô∏è Max recognition attempts reached")
            detectionState = DetectionState.PERSON_IDENTIFIED
            voiceService.speak("Impossible de reconna√Ætre la personne.")
            hasAnnouncedPerson = true
            return
        }
        
        // Transition: ‚Üí RECOGNIZING
        detectionState = DetectionState.RECOGNIZING
        recognitionAttempts++
        lastRecognitionAt = currentTime
        
        Log.d("AutoBlind", "üîç Starting recognition (attempt $recognitionAttempts)...")
        
        // Appel API (silencieux)
        val result = faceRepository.recognizeFaces(bitmap, saveToHistory = true)
        
        result.onSuccess { response ->
            detectionCount++
            
            if (response.facesDetected == 0) {
                Log.w("AutoBlind", "‚ö†Ô∏è No faces in recognition result")
                
                // Retry si pas au max
                if (recognitionAttempts < MAX_RECOGNITION_ATTEMPTS) {
                    delay(300)
                    currentBitmap?.let { startRecognition(it) }
                } else {
                    resetToIdle()
                }
                return@onSuccess
            }
            
            val person = response.recognizedPersons.firstOrNull()
            
            if (person == null) {
                // Personne inconnue
                announceUnknownPerson()
                currentPersonId = "unknown"
                currentPersonName = "Inconnu"
                hasAnnouncedPerson = true
                detectionState = DetectionState.PERSON_IDENTIFIED
                return@onSuccess
            }
            
            // V√©rifier si c'est la m√™me personne qu'avant
            if (person.personId == currentPersonId && hasAnnouncedPerson) {
                Log.d("AutoBlind", "üîá Same person (${person.name}), staying silent...")
                detectionState = DetectionState.PERSON_IDENTIFIED
                return@onSuccess
            }
            
            // Nouvelle personne ou premi√®re annonce
            currentPersonId = person.personId
            currentPersonName = person.name
            hasAnnouncedPerson = true
            detectionState = DetectionState.PERSON_IDENTIFIED
            
            if (person.isRecognized && person.confidence >= MIN_CONFIDENCE) {
                announceRecognizedPerson(person)
            } else {
                announceUnknownPerson()
            }
        }.onFailure { error ->
            Log.e("AutoBlind", "‚ùå Recognition failed: ${error.message}")
            
            // Retry si pas au max
            if (recognitionAttempts < MAX_RECOGNITION_ATTEMPTS) {
                delay(500)
                currentBitmap?.let { startRecognition(it) }
            } else {
                detectionState = DetectionState.PERSON_IDENTIFIED
                voiceService.speak("Erreur de reconnaissance.")
                hasAnnouncedPerson = true
            }
        }
    }
    
    // ‚úÖ Traitement frame cam√©ra (MACHINE √Ä √âTATS)
    fun onCameraFrame(bitmap: Bitmap) {
        // ‚úÖ OPTIMISATION: Ne pas traiter si l'analyse est d√©sactiv√©e
        if (!cameraAnalysisEnabled) {
            return
        }
        
        scope.launch {
            val currentTime = System.currentTimeMillis()
            
            // Limiter framerate d'analyse
            if (currentTime - lastAnalysisTime < FRAME_ANALYSIS_INTERVAL_MS) {
                return@launch
            }
            lastAnalysisTime = currentTime
            
            // D√©tection locale (rapide) avec position
            val (hasFace, facePosition) = detectFaceLocally(bitmap)
            
            when (detectionState) {
                DetectionState.IDLE -> {
                    if (hasFace) {
                        // Transition: IDLE ‚Üí FACE_DETECTED
                        faceFirstDetectedAt = currentTime
                        faceLastSeenAt = currentTime
                        consecutiveramesWithFace = 1
                        consecutiveFramesWithoutFace = 0
                        lastFacePosition = facePosition  // ‚úÖ Sauvegarder position
                        detectionState = DetectionState.FACE_DETECTED
                        Log.d("AutoBlind", "üü¢ Face detected, waiting for stability...")
                    }
                }
                
                DetectionState.FACE_DETECTED -> {
                    if (hasFace) {
                        consecutiveramesWithFace++
                        consecutiveFramesWithoutFace = 0  // ‚úÖ Reset counter
                        faceLastSeenAt = currentTime
                        lastFacePosition = facePosition  // ‚úÖ Mettre √† jour position
                        
                        // V√©rifier stabilit√©
                        val stableDuration = currentTime - faceFirstDetectedAt
                        if (stableDuration >= STABILITY_THRESHOLD_MS && !hasAnnouncedDetection) {
                            // Transition: FACE_DETECTED ‚Üí FACE_STABLE
                            detectionState = DetectionState.FACE_STABLE
                            hasAnnouncedDetection = true
                            voiceService.speak("Visage d√©tect√©.")
                            Log.d("AutoBlind", "üü¢ Face stable (${stableDuration}ms), announcing...")
                            
                            // Lancer reconnaissance imm√©diatement
                            startRecognition(bitmap)
                        }
                    } else {
                        consecutiveFramesWithoutFace++
                        
                        // ‚úÖ Tol√©rance am√©lior√©e: accepter quelques frames sans visage
                        if (consecutiveFramesWithoutFace > MAX_FRAMES_WITHOUT_FACE_BEFORE_RESET) {
                            Log.d("AutoBlind", "‚ö†Ô∏è Face disappeared before stability ($consecutiveFramesWithoutFace frames without face)")
                            resetToIdle()
                        }
                    }
                }
                
                DetectionState.FACE_STABLE -> {
                    // √âtat transitoire, g√©r√© par startRecognition
                    if (!hasFace) {
                        consecutiveFramesWithoutFace++
                        if (consecutiveFramesWithoutFace > 10) {
                            resetToIdle()
                        }
                    } else {
                        faceLastSeenAt = currentTime
                        lastFacePosition = facePosition
                    }
                }
                
                DetectionState.RECOGNIZING -> {
                    // Reconnaissance en cours, ne rien faire
                    if (!hasFace) {
                        consecutiveFramesWithoutFace++
                        
                        // Si visage dispara√Æt pendant reconnaissance
                        if (consecutiveFramesWithoutFace > 10) {
                            Log.w("AutoBlind", "‚ö†Ô∏è Face disappeared during recognition")
                            resetToIdle()
                        }
                    } else {
                        faceLastSeenAt = currentTime
                        consecutiveFramesWithoutFace = 0
                        lastFacePosition = facePosition
                    }
                }
                
                DetectionState.PERSON_IDENTIFIED -> {
                    // Annonce d√©j√† faite, en cooldown
                    if (hasFace) {
                        faceLastSeenAt = currentTime
                        consecutiveFramesWithoutFace = 0
                        
                        // ‚úÖ NOUVEAU: D√©tecter si le visage a chang√© de position (nouvelle personne ou changement de direction)
                        if (hasFacePositionChanged(lastFacePosition, facePosition)) {
                            Log.d("AutoBlind", "üîÑ Face position changed significantly - resetting for new detection!")
                            // Reset et red√©marrer la d√©tection imm√©diatement
                            resetToIdle()
                            // Imm√©diatement passer √† FACE_DETECTED (pas besoin d'attendre)
                            faceFirstDetectedAt = currentTime
                            faceLastSeenAt = currentTime
                            consecutiveramesWithFace = 1
                            lastFacePosition = facePosition
                            detectionState = DetectionState.FACE_DETECTED
                        } else {
                            // M√™me personne, mettre √† jour position
                            lastFacePosition = facePosition
                            
                            // ‚úÖ Si cooldown termin√© et visage toujours l√†, permettre nouvelle reconnaissance
                            val timeSinceLastRecognition = currentTime - lastRecognitionAt
                            if (timeSinceLastRecognition >= RECOGNITION_COOLDOWN_MS && !hasAnnouncedPerson) {
                                // R√©initialiser pour permettre nouvelle reconnaissance
                                recognitionAttempts = 0
                                hasAnnouncedPerson = false
                                currentPersonId = null
                                currentPersonName = null
                                Log.d("AutoBlind", "üîÑ Cooldown finished, ready for new recognition")
                            }
                        }
                    } else {
                        consecutiveFramesWithoutFace++
                        
                        // ‚úÖ Si visage dispara√Æt bri√®vement (changement de direction), reset rapide
                        val timeSinceLastSeen = currentTime - faceLastSeenAt
                        if (timeSinceLastSeen >= DISAPPEAR_THRESHOLD_MS) {
                            Log.d("AutoBlind", "üîÑ Person left (${timeSinceLastSeen}ms without face), resetting...")
                            resetToIdle()
                        } else if (consecutiveFramesWithoutFace > MAX_FRAMES_WITHOUT_FACE_BEFORE_RESET) {
                            // ‚úÖ Reset m√™me si pas encore 2s, si beaucoup de frames sans visage (changement rapide)
                            Log.d("AutoBlind", "üîÑ Many frames without face ($consecutiveFramesWithoutFace), resetting...")
                            resetToIdle()
                        }
                    }
                }
            }
        }
    }
    
    // ‚úÖ Permissions
    val permissionsState = rememberMultiplePermissionsState(
        permissions = listOf(
            Manifest.permission.RECORD_AUDIO,
            Manifest.permission.CAMERA
        )
    )

    // ‚úÖ G√©rer les commandes vocales
    LaunchedEffect(recognizedText) {
        if (recognizedText.isNotEmpty()) {
            val lower = recognizedText.lowercase()

            when {
                lower.contains("quitter") || lower.contains("sortir") ||
                        lower.contains("exit") || lower.contains("quit") -> {
                    showExitDialog = true
                }
                else -> {
                    // ‚úÖ V√©rifier si syst√®me occup√©
                    if (isProcessingCommand) {
                        Log.d("AutoBlind", "‚è≥ System busy, ignoring command: $recognizedText")
                        voiceService.speak("Un instant, traitement en cours.")
                    } else {
                        // Envoyer au serveur
                        Log.d("AutoBlind", "üì§ Sending voice command: $recognizedText")
                        isProcessingCommand = true // üîí Verrouiller
                        wsClient.sendVoiceCommand(recognizedText)
                    }
                    // ‚úÖ R√©initialiser le texte reconnu pour √©viter les traitements multiples
                    // Le service red√©marrera automatiquement l'√©coute
                }
            }
        }
    }

    // ‚úÖ Contr√¥leur capture
    val captureController = remember { CaptureController() }

    // ‚úÖ G√©rer les r√©ponses du serveur (Commandes imm√©diates)
    LaunchedEffect(voiceResponse) {
        voiceResponse?.let { response ->
            Log.d("AutoBlind", "üì® Voice response received: action=${response.action}, message=${response.message}")
            isProcessingCommand = false // üîì D√©verrouiller
            
            response.speakText?.let { 
                voiceService.speak(it) 
            }

            when (response.action) {
                "navigate-back" -> {
                    delay(500)
                    navController.popBackStack()
                }
                "capture-photo" -> {
                    // ‚úÖ OPTIMISATION: Capture unique avec contr√¥le
                    if (!alreadyCaptured) {
                        Log.d("AutoBlind", "üì∏ Capture photo requested (Action) - First capture")
                        alreadyCaptured = true
                        
                        // ‚úÖ Pause l'analyse cam√©ra pour am√©liorer la r√©activit√©
                        cameraAnalysisEnabled = false
                        Log.d("AutoBlind", "‚è∏Ô∏è Camera analysis paused for capture")
                        
                        delay(500) // Petit d√©lai pour que le TTS commence
                        captureController.capturePhoto?.invoke()
                    } else {
                        Log.d("AutoBlind", "‚ö†Ô∏è Capture already done, ignoring request")
                    }
                }
                else -> {
                    // G√©rer la navigation si pr√©sente
                    response.navigation?.let { dest ->
                        Log.d("AutoBlind", "üöÄ Navigating to: $dest (from response)")
                        navController.navigate(dest)
                    }
                }
            }
        }
    }

    // ‚úÖ NOUVEAU: G√©rer les instructions de d√©tection Yolo (R√©sultats)
    LaunchedEffect(voiceInstruction) {
        voiceInstruction?.let { instruction ->
            Log.d("AutoBlind", "üîä Received voice instruction: ${instruction.action} -> ${instruction.text}")
            isProcessingCommand = false // üîì D√©verrouiller (si c'√©tait une commande qui a g√©n√©r√© √ßa)
            
            // Par exemple: "Photo enregistr√©e. D√©tection termin√©e. 2 objets d√©tect√©s: bottle, person."
            voiceService.speak(instruction.text) {
                // ‚úÖ Une fois que le r√©sultat est lu, on peut reprendre l'analyse
                if (!cameraAnalysisEnabled) {
                    Log.d("AutoBlind", "‚ñ∂Ô∏è Resuming camera analysis after result announcement")
                    cameraAnalysisEnabled = true
                    alreadyCaptured = false // Pr√™t pour une nouvelle capture si demand√©e
                }
                
                // G√©rer la navigation si pr√©sente dans l'instruction
                instruction.navigation?.let { dest ->
                    Log.d("AutoBlind", "üöÄ Navigating to: $dest (from instruction)")
                    navController.navigate(dest)
                }
            }
        }
    }

    // ‚úÖ NOUVEAU: G√©rer les changements de connexion WebSocket (Feedback vocal + Resilience)
    LaunchedEffect(connectionState) {
        when (connectionState) {
            ConnectionState.CONNECTED -> {
                voiceService.speak("Serveur connect√©.")
                // ‚úÖ RESILIENCE: Si on se reconnecte, on r√©active tout par s√©curit√©
                cameraAnalysisEnabled = true
                alreadyCaptured = false
            }
            ConnectionState.DISCONNECTED -> {
                if (isCameraActive) {
                    voiceService.speak("Connexion au serveur perdue.")
                    // ‚úÖ RESILIENCE: Si on perd la connexion pendant une capture, 
                    // on r√©active l'analyse pour ne pas rester bloqu√©
                    cameraAnalysisEnabled = true
                    alreadyCaptured = false
                }
            }
            ConnectionState.ERROR -> {
                voiceService.speak("Erreur de connexion au serveur.")
                cameraAnalysisEnabled = true
                alreadyCaptured = false
            }
            else -> {}
        }
    }

    // ‚úÖ NOUVEAU: Activation automatique du MICRO d√®s que possible
    // (Attendre que le service soit pr√™t ET que le serveur soit connect√©)
    LaunchedEffect(connectionState, isVoiceReady) {
        if (connectionState == ConnectionState.CONNECTED && isVoiceReady && !isListening) {
            Log.d("AutoBlind", "üé§ Auto-starting microphone...")
            delay(1000) // Petit d√©lai pour laisser passer le "Serveur connect√©"
            voiceService.startListening()
            isListening = true
        }
    }

    // ‚úÖ Initialisation (cam√©ra uniquement, le micro est g√©r√© par l'effet ci-dessus)
    LaunchedEffect(permissionsState.allPermissionsGranted) {
        if (permissionsState.allPermissionsGranted) {
            if (wsClient.shouldReconnect()) {
                delay(500)
                wsClient.connect()
            } else {
                Log.d("AutoBlind", "‚úÖ Socket already connected, reusing")
            }
            delay(1000)
            isCameraActive = true
        }
    }

    // ‚úÖ Cleanup: Disconnect ONLY when leaving AutoBlind mode
    DisposableEffect(Unit) {
        onDispose {
            Log.d("AutoBlind", "üö™ Exiting AutoBlind mode - Cleaning up...")
            isCameraActive = false
            voiceService.cleanup()
            wsClient.disconnect() // ‚úÖ Explicit disconnect as requested
        }
    }

    Box(modifier = Modifier.fillMaxSize()) {
        // ‚úÖ Cam√©ra avec d√©tection
        if (isCameraActive) {
            SmartCameraPreview(
                modifier = Modifier.fillMaxSize(),
                captureController = captureController,
                onFrameAnalyzed = { bitmap ->
                    currentBitmap = bitmap
                    onCameraFrame(bitmap) // Flux visage
                },
                onPhotoCaptured = { hdBitmap ->
                    // Flux haute qualit√© pour YOLO
                    Log.d("AutoBlind", "‚úÖ HD Photo Captured. Sending to backend...")
                    
                    scope.launch {
                        if (wsClient.connectionState.value == ConnectionState.CONNECTED) {
                            wsClient.sendPhotoForProcessing(hdBitmap)
                        } else {
                            Log.e("AutoBlind", "‚ùå Cannot send: WebSocket not connected")
                            voiceService.speak("Erreur: connexion perdue.")
                            cameraAnalysisEnabled = true
                            alreadyCaptured = false
                        }
                    }
                },
                analysisEnabled = cameraAnalysisEnabled
            )

        } else {
            Box(
                modifier = Modifier
                    .fillMaxSize()
                    .background(Color(0xFF212121)),
                contentAlignment = Alignment.Center
            ) {
                Text(
                    text = "Cam√©ra d√©sactiv√©e",
                    color = Color.White,
                    fontSize = 20.sp
                )
            }
        }

        // ‚úÖ Overlay d'informations
        Column(
            modifier = Modifier
                .fillMaxSize()
                .padding(20.dp)
        ) {
            // En-t√™te
            Card(
                modifier = Modifier.fillMaxWidth(),
                colors = CardDefaults.cardColors(
                    containerColor = Color.Black.copy(alpha = 0.7f)
                ),
                shape = RoundedCornerShape(16.dp)
            ) {
                Column(
                    modifier = Modifier.padding(16.dp),
                    verticalArrangement = Arrangement.spacedBy(8.dp)
                ) {
                    Row(
                        modifier = Modifier.fillMaxWidth(),
                        horizontalArrangement = Arrangement.SpaceBetween,
                        verticalAlignment = Alignment.CenterVertically
                    ) {
                        Text(
                            text = "Mode Auto",
                            fontSize = 20.sp,
                            fontWeight = FontWeight.Bold,
                            color = Color.White
                        )

                        Row(
                            horizontalArrangement = Arrangement.spacedBy(8.dp),
                            verticalAlignment = Alignment.CenterVertically
                        ) {
                            // √âtat WebSocket
                            Box(
                                modifier = Modifier
                                    .size(12.dp)
                                    .background(
                                        color = when (connectionState) {
                                            ConnectionState.CONNECTED -> Color(0xFF4CAF50)
                                            ConnectionState.CONNECTING -> Color(0xFFFFC107)
                                            else -> Color(0xFFF44336)
                                        },
                                        shape = CircleShape
                                    )
                            )

                            // √âtat micro
                            if (isListening) {
                                Icon(
                                    imageVector = Icons.Default.Mic,
                                    contentDescription = "Listening",
                                    tint = Color(0xFF4CAF50),
                                    modifier = Modifier.size(20.dp)
                                )
                            }
                        }
                    }

                    // Indicateur de d√©tection (bas√© sur machine √† √©tats)
                    Row(
                        verticalAlignment = Alignment.CenterVertically,
                        horizontalArrangement = Arrangement.spacedBy(8.dp)
                    ) {
                        val (icon, text, color) = when (detectionState) {
                            DetectionState.IDLE -> Triple(Icons.Default.Search, "Recherche en cours...", Color.White)
                            DetectionState.FACE_DETECTED -> Triple(Icons.Default.Face, "Visage en cours de stabilisation...", Color(0xFFFFC107))
                            DetectionState.FACE_STABLE -> Triple(Icons.Default.Face, "Visage stable", Color(0xFF4CAF50))
                            DetectionState.RECOGNIZING -> Triple(Icons.Default.Face, "Reconnaissance en cours...", Color(0xFF2196F3))
                            DetectionState.PERSON_IDENTIFIED -> {
                                val personName = currentPersonName ?: "Personne"
                                Triple(Icons.Default.Face, personName, Color(0xFF4CAF50))
                            }
                        }
                        
                        Icon(
                            imageVector = icon,
                            contentDescription = null,
                            tint = color,
                            modifier = Modifier.size(20.dp)
                        )
                        Text(
                            text = text,
                            color = Color.White,
                            fontSize = 14.sp
                        )
                    }

                    Text(
                        text = "D√©tections: $detectionCount",
                        color = Color.White.copy(alpha = 0.7f),
                        fontSize = 12.sp
                    )
                }
            }

            Spacer(modifier = Modifier.weight(1f))

            // ‚úÖ Commandes vocales
            if (recognizedText.isNotEmpty()) {
                Card(
                    modifier = Modifier.fillMaxWidth(),
                    colors = CardDefaults.cardColors(
                        containerColor = Color.Black.copy(alpha = 0.7f)
                    ),
                    shape = RoundedCornerShape(16.dp)
                ) {
                    Column(modifier = Modifier.padding(12.dp)) {
                        Text(
                            text = "Commande:",
                            fontSize = 12.sp,
                            fontWeight = FontWeight.Bold,
                            color = Color(0xFF4CAF50)
                        )
                        Text(
                            text = recognizedText,
                            fontSize = 14.sp,
                            color = Color.White
                        )
                    }
                }
            }

            Spacer(modifier = Modifier.height(16.dp))

            // ‚úÖ Boutons de contr√¥le
            Row(
                modifier = Modifier.fillMaxWidth(),
                horizontalArrangement = Arrangement.SpaceEvenly
            ) {
                // Toggle micro
                FloatingActionButton(
                    onClick = {
                        if (isListening) {
                            voiceService.stopListening()
                            isListening = false
                            voiceService.speak("Micro d√©sactiv√©")
                        } else {
                            voiceService.startListening()
                            isListening = true
                            voiceService.speak("Micro activ√©")
                        }
                    },
                    containerColor = if (isListening) Color(0xFF4CAF50) else Color.White
                ) {
                    Icon(
                        imageVector = if (isListening) Icons.Default.Mic else Icons.Default.MicOff,
                        contentDescription = "Toggle Mic",
                        tint = if (isListening) Color.White else Color.Black
                    )
                }

                // Toggle cam√©ra
                FloatingActionButton(
                    onClick = {
                        if (isCameraActive) {
                            isCameraActive = false
                            voiceService.speak("Cam√©ra d√©sactiv√©e")
                        } else {
                            isCameraActive = true
                            voiceService.speak("Cam√©ra activ√©e")
                        }
                    },
                    containerColor = if (isCameraActive) Color(0xFF4CAF50) else Color.White
                ) {
                    Icon(
                        imageVector = if (isCameraActive) Icons.Default.Videocam else Icons.Default.VideocamOff,
                        contentDescription = "Toggle Camera",
                        tint = if (isCameraActive) Color.White else Color.Black
                    )
                }

                // Aide
                FloatingActionButton(
                    onClick = {
                        wsClient.requestHelp()
                    },
                    containerColor = Color.White
                ) {
                    Icon(
                        imageVector = Icons.Default.Help,
                        contentDescription = "Help",
                        tint = Color.Black
                    )
                }

                // Quitter
                FloatingActionButton(
                    onClick = {
                        showExitDialog = true
                    },
                    containerColor = Color(0xFFF44336)
                ) {
                    Icon(
                        imageVector = Icons.Default.ExitToApp,
                        contentDescription = "Exit",
                        tint = Color.White
                    )
                }
            }
        }
    }

    // ‚úÖ Dialog de permissions
    if (!permissionsState.allPermissionsGranted) {
        AlertDialog(
            onDismissRequest = { },
            title = { Text("Permissions requises") },
            text = {
                Column {
                    Text("Cette fonctionnalit√© n√©cessite :")
                    Spacer(modifier = Modifier.height(8.dp))
                    Text("‚Ä¢ Microphone : commandes vocales")
                    Text("‚Ä¢ Cam√©ra : reconnaissance faciale")
                }
            },
            confirmButton = {
                TextButton(
                    onClick = {
                        permissionsState.launchMultiplePermissionRequest()
                    }
                ) {
                    Text("Autoriser")
                }
            }
        )
    }

    // ‚úÖ Dialog de sortie
    if (showExitDialog) {
        AlertDialog(
            onDismissRequest = { showExitDialog = false },
            title = { Text("Quitter le mode automatique ?") },
            text = { Text("La cam√©ra et le micro seront d√©sactiv√©s.") },
            confirmButton = {
                TextButton(
                    onClick = {
                        scope.launch {
                            isCameraActive = false
                            voiceService.cleanup()
                            wsClient.disconnect() // ‚úÖ D√©connexion explicite
                            voiceService.speak("Mode automatique d√©sactiv√©")
                            delay(1000)
                            navController.popBackStack()
                        }
                    }
                ) {
                    Text("Oui, quitter")
                }
            },
            dismissButton = {
                TextButton(onClick = { showExitDialog = false }) {
                    Text("Annuler")
                }
            }
        )
    }
}


// ‚úÖ Contr√¥leur pour d√©lencher la capture depuis le parent
class CaptureController {
    var capturePhoto: (() -> Unit)? = null
}

@Composable
fun SmartCameraPreview(
    modifier: Modifier = Modifier,
    captureController: CaptureController,
    onFrameAnalyzed: (Bitmap) -> Unit, // Pour le Visage (basse res)
    onPhotoCaptured: (Bitmap) -> Unit, // Pour YOLO (haute res)
    analysisEnabled: Boolean = true // ‚úÖ Contr√¥le dynamique de l'analyse
) {
    val context = LocalContext.current
    val lifecycleOwner = LocalLifecycleOwner.current

    val previewView = remember { PreviewView(context) }
    
    // ‚úÖ R√©f√©rence mutable pour contr√¥ler l'analyseur
    var imageAnalyzerRef by remember { mutableStateOf<ImageAnalysis?>(null) }

    AndroidView(
        factory = { previewView },
        modifier = modifier
    )
    
    // ‚úÖ Observer les changements d'analysisEnabled pour activer/d√©sactiver l'analyse
    LaunchedEffect(analysisEnabled) {
        imageAnalyzerRef?.let { analyzer ->
            if (analysisEnabled) {
                Log.d("SmartCamera", "‚ñ∂Ô∏è Resuming camera analysis")
                analyzer.clearAnalyzer()
                analyzer.setAnalyzer(ContextCompat.getMainExecutor(context)) { imageProxy ->
                    val bitmap = imageProxy.toBitmap()
                    onFrameAnalyzed(bitmap)
                    imageProxy.close()
                }
            } else {
                Log.d("SmartCamera", "‚è∏Ô∏è Pausing camera analysis")
                analyzer.clearAnalyzer()
            }
        }
    }

    LaunchedEffect(Unit) {
        val cameraProviderFuture = ProcessCameraProvider.getInstance(context)
        val cameraProvider = cameraProviderFuture.get()

        // 1. Preview
        val preview = Preview.Builder().build().also {
            it.setSurfaceProvider(previewView.surfaceProvider)
        }

        // 2. Image Analysis (Visage - Stream Realtime) - OPTIMIS√â
        val imageAnalyzer = ImageAnalysis.Builder()
            .setTargetResolution(android.util.Size(320, 240))  // ‚úÖ R√©solution r√©duite pour performance
            .setBackpressureStrategy(ImageAnalysis.STRATEGY_KEEP_ONLY_LATEST)  // ‚úÖ Garder seulement la derni√®re frame
            .build()
            .also { analyzer ->
                analyzer.setAnalyzer(ContextCompat.getMainExecutor(context)) { imageProxy ->
                    val bitmap = imageProxy.toBitmap()
                    onFrameAnalyzed(bitmap)
                    imageProxy.close()  // ‚úÖ Lib√©ration correcte des ressources
                }
            }
        
        imageAnalyzerRef = imageAnalyzer  // ‚úÖ Sauvegarder la r√©f√©rence

        // 3. Image Capture (YOLO - Haute Qualit√©)
        val imageCapture = ImageCapture.Builder()
            .setCaptureMode(ImageCapture.CAPTURE_MODE_MINIMIZE_LATENCY)
            .build()

        // Lier le contr√¥leur
        captureController.capturePhoto = {
            imageCapture.takePicture(
                ContextCompat.getMainExecutor(context),
                object : ImageCapture.OnImageCapturedCallback() {
                    override fun onCaptureSuccess(image: ImageProxy) {
                        val bitmap = image.toBitmap()
                        onPhotoCaptured(bitmap)
                        image.close() // Important!
                    }

                    override fun onError(exception: ImageCaptureException) {
                        Log.e("SmartCamera", "Capture failed: ${exception.message}", exception)
                    }
                }
            )
        }

        val cameraSelector = CameraSelector.DEFAULT_BACK_CAMERA

        try {
            cameraProvider.unbindAll()
            // Bind les 3 cas d'utilisation ensemble
            cameraProvider.bindToLifecycle(
                lifecycleOwner, 
                cameraSelector, 
                preview, 
                imageAnalyzer, 
                imageCapture
            )
        } catch (e: Exception) {
            Log.e("SmartCamera", "Use case binding failed", e)
        }
    }
}

// ‚úÖ Extension helper - Handles both YUV (ImageAnalysis) and JPEG (ImageCapture)
fun ImageProxy.toBitmap(): Bitmap {
    return when (format) {
        ImageFormat.JPEG -> {
            // ImageCapture returns JPEG - decode directly
            val buffer = planes[0].buffer
            val bytes = ByteArray(buffer.remaining())
            buffer.get(bytes)
            BitmapFactory.decodeByteArray(bytes, 0, bytes.size)
        }
        ImageFormat.YUV_420_888 -> {
            // ImageAnalysis returns YUV - convert to NV21 then JPEG
            val yBuffer = planes[0].buffer
            val uBuffer = planes[1].buffer
            val vBuffer = planes[2].buffer

            val ySize = yBuffer.remaining()
            val uSize = uBuffer.remaining()
            val vSize = vBuffer.remaining()

            val nv21 = ByteArray(ySize + uSize + vSize)
            yBuffer.get(nv21, 0, ySize)
            vBuffer.get(nv21, ySize, vSize)
            uBuffer.get(nv21, ySize + vSize, uSize)

            val yuvImage = YuvImage(nv21, ImageFormat.NV21, width, height, null)
            val out = ByteArrayOutputStream()
            yuvImage.compressToJpeg(Rect(0, 0, width, height), 100, out)
            val imageBytes = out.toByteArray()
            BitmapFactory.decodeByteArray(imageBytes, 0, imageBytes.size)
        }
        else -> {
            // Fallback for unknown formats
            Log.e("ImageProxy", "Unsupported image format: $format")
            throw IllegalArgumentException("Unsupported image format: $format")
        }
    }
}
