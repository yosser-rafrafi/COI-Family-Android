package tn.esprit.coidam.screens

import android.Manifest
import android.graphics.Bitmap
import android.graphics.BitmapFactory
import android.graphics.ImageFormat
import android.graphics.Rect
import android.graphics.YuvImage
import android.util.Log
import androidx.camera.core.CameraSelector
import androidx.camera.core.ImageAnalysis
import androidx.camera.core.ImageProxy
import androidx.camera.core.Preview
import androidx.camera.lifecycle.ProcessCameraProvider
import androidx.camera.view.PreviewView
import androidx.compose.foundation.background
import androidx.compose.foundation.layout.*
import androidx.compose.foundation.shape.CircleShape
import androidx.compose.foundation.shape.RoundedCornerShape
import androidx.compose.material.icons.Icons
import androidx.compose.material.icons.filled.*
import androidx.compose.material3.*
import androidx.compose.runtime.*
import androidx.compose.ui.Alignment
import androidx.compose.ui.Modifier
import androidx.compose.ui.graphics.Color
import androidx.compose.ui.platform.LocalContext
import androidx.compose.ui.text.font.FontWeight
import androidx.compose.ui.unit.dp
import androidx.compose.ui.unit.sp
import androidx.compose.ui.viewinterop.AndroidView
import androidx.core.content.ContextCompat
import androidx.lifecycle.compose.LocalLifecycleOwner
import androidx.navigation.NavController
import com.google.accompanist.permissions.ExperimentalPermissionsApi
import com.google.accompanist.permissions.rememberMultiplePermissionsState
import kotlinx.coroutines.delay
import kotlinx.coroutines.launch
import tn.esprit.coidam.data.api.VoiceCommandService
import tn.esprit.coidam.data.api.VoiceWebSocketClient
import tn.esprit.coidam.data.local.TokenManager
import tn.esprit.coidam.data.models.Enums.ConnectionState
import tn.esprit.coidam.data.models.Enums.DetectionState
import tn.esprit.coidam.data.repository.FaceRecognitionRepository
import com.google.mlkit.vision.common.InputImage
import com.google.mlkit.vision.face.FaceDetection
import com.google.mlkit.vision.face.FaceDetectorOptions
import java.io.ByteArrayOutputStream
import kotlin.coroutines.resume
import kotlin.coroutines.suspendCoroutine

@OptIn(ExperimentalPermissionsApi::class)
@Composable
fun AutoBlindScreen(navController: NavController) {
    val context = LocalContext.current
    val scope = rememberCoroutineScope()
    val tokenManager = remember { TokenManager(context) }

    // ‚úÖ Services (using singleton)
    val voiceService = remember { VoiceCommandService(context) }
    val wsClient = remember { VoiceWebSocketClient.getInstance(context) }
    val faceRepository = remember { FaceRecognitionRepository(context) }

    // ‚úÖ √âtats WebSocket et Voice
    val isVoiceReady by voiceService.isReady.collectAsState()
    val recognizedText by voiceService.recognizedText.collectAsState()
    val connectionState by wsClient.connectionState.collectAsState()
    val voiceInstruction by wsClient.voiceInstruction.collectAsState()
    val voiceResponse by wsClient.voiceResponse.collectAsState()

    // ‚úÖ √âtats UI
    var isListening by remember { mutableStateOf(false) }
    var isCameraActive by remember { mutableStateOf(false) }
    var showExitDialog by remember { mutableStateOf(false) }
    var currentBitmap by remember { mutableStateOf<Bitmap?>(null) }
    
    // ‚úÖ MACHINE √Ä √âTATS - Variables d'√©tat
    var detectionState by remember { mutableStateOf(DetectionState.IDLE) }
    var faceFirstDetectedAt by remember { mutableLongStateOf(0L) }
    var faceLastSeenAt by remember { mutableLongStateOf(0L) }
    var lastRecognitionAt by remember { mutableLongStateOf(0L) }
    var lastAnalysisTime by remember { mutableLongStateOf(0L) }
    var currentPersonId by remember { mutableStateOf<String?>(null) }
    var currentPersonName by remember { mutableStateOf<String?>(null) }
    var hasAnnouncedDetection by remember { mutableStateOf(false) }
    var hasAnnouncedPerson by remember { mutableStateOf(false) }
    var recognitionAttempts by remember { mutableIntStateOf(0) }
    var consecutiveFramesWithFace by remember { mutableIntStateOf(0) }
    var consecutiveFramesWithoutFace by remember { mutableIntStateOf(0) }
    var detectionCount by remember { mutableIntStateOf(0) }
    var lastFacePosition by remember { mutableStateOf<android.graphics.RectF?>(null) }  // ‚úÖ Tracker position du visage
    
    // ‚úÖ Configuration (ULTRA-OPTIMIS√â pour ML Kit intermittent)
    val STABILITY_THRESHOLD_MS = 500L              // ‚úÖ R√©duit √† 500ms pour d√©clenchement rapide
    val DISAPPEAR_THRESHOLD_MS = 5000L             // 5s sans visage = reset
    val RECOGNITION_COOLDOWN_MS = 3000L            // 3s entre deux reconnaissances
    val MIN_CONFIDENCE = 0.3f                      // Confiance minimum pour annoncer
    val MAX_RECOGNITION_ATTEMPTS = 3               // Maximum 3 captures
    val FRAME_ANALYSIS_INTERVAL_MS = 300L          // 300ms entre analyses
    val MAX_FRAMES_WITHOUT_FACE_BEFORE_RESET = 15  // ‚úÖ Tol√©rance augment√©e: 15 frames (~4.5s) pour ML Kit intermittent
    
    // ‚úÖ ML Kit Face Detector
    val faceDetector = remember {
        val options = FaceDetectorOptions.Builder()
            .setPerformanceMode(FaceDetectorOptions.PERFORMANCE_MODE_FAST)
            .setMinFaceSize(0.15f)
            .setLandmarkMode(FaceDetectorOptions.LANDMARK_MODE_NONE)
            .setClassificationMode(FaceDetectorOptions.CLASSIFICATION_MODE_NONE)
            .build()
        FaceDetection.getClient(options)
    }
    
    // ‚úÖ D√©tection locale de visage (ML Kit) avec tracking de position
    suspend fun detectFaceLocally(bitmap: Bitmap): Pair<Boolean, android.graphics.RectF?> {
        return suspendCoroutine { continuation ->
            val image = InputImage.fromBitmap(bitmap, 0)
            faceDetector.process(image)
                .addOnSuccessListener { faces ->
                    if (faces.isNotEmpty()) {
                        // Retourner le bounding box du premier visage
                        val face = faces[0]
                        val bounds = face.boundingBox
                        val rectF = android.graphics.RectF(
                            bounds.left.toFloat(),
                            bounds.top.toFloat(),
                            bounds.right.toFloat(),
                            bounds.bottom.toFloat()
                        )
                        continuation.resume(Pair(true, rectF))
                    } else {
                        continuation.resume(Pair(false, null))
                    }
                }
                .addOnFailureListener { e ->
                    Log.e("AutoBlind", "Face detection failed: ${e.message}")
                    continuation.resume(Pair(false, null))
                }
        }
    }
    
    // ‚úÖ V√©rifier si le visage a significativement chang√© de position
    fun hasFacePositionChanged(oldRect: android.graphics.RectF?, newRect: android.graphics.RectF?): Boolean {
        if (oldRect == null || newRect == null) return true
        
        // Calculer le d√©placement du centre
        val oldCenterX = (oldRect.left + oldRect.right) / 2
        val oldCenterY = (oldRect.top + oldRect.bottom) / 2
        val newCenterX = (newRect.left + newRect.right) / 2
        val newCenterY = (newRect.top + newRect.bottom) / 2
        
        val deltaX = kotlin.math.abs(newCenterX - oldCenterX)
        val deltaY = kotlin.math.abs(newCenterY - oldCenterY)
        
        // Utiliser la largeur du visage comme r√©f√©rence (30% de d√©placement)
        val faceWidth = oldRect.right - oldRect.left
        val threshold = faceWidth * 0.3f
        
        val changed = deltaX > threshold || deltaY > threshold
        if (changed) {
            Log.d("AutoBlind", "üìç Face moved: deltaX=$deltaX, deltaY=$deltaY, threshold=$threshold")
        }
        
        return changed
    }
    
    // ‚úÖ Reset vers IDLE
    fun resetToIdle() {
        Log.d("AutoBlind", "üîÑ Resetting to IDLE state")
        detectionState = DetectionState.IDLE
        faceFirstDetectedAt = 0L
        faceLastSeenAt = 0L
        consecutiveFramesWithFace = 0
        consecutiveFramesWithoutFace = 0
        hasAnnouncedDetection = false
        hasAnnouncedPerson = false
        recognitionAttempts = 0
        currentPersonId = null
        currentPersonName = null
        lastFacePosition = null  // ‚úÖ Reset position
    }
    
    // ‚úÖ Annonce personne reconnue
    fun announceRecognizedPerson(person: tn.esprit.coidam.data.models.FaceRecognition.RecognizedPerson) {
        val confidencePercent = (person.confidence * 100).toInt()
        val announcement = buildString {
            append("C'est ${person.name}")
            person.relation?.let { append(", votre $it") }
            append(". Confiance $confidencePercent pour cent.")
        }
        Log.d("AutoBlind", "üó£Ô∏è Announcing: $announcement")
        voiceService.speak(announcement)
    }
    
    // ‚úÖ Annonce personne inconnue
    fun announceUnknownPerson() {
        val announcement = "Personne inconnue devant vous."
        Log.d("AutoBlind", "üó£Ô∏è Announcing: $announcement")
        voiceService.speak(announcement)
    }
    
    // ‚úÖ Reconnaissance faciale
    suspend fun startRecognition(bitmap: Bitmap) {
        val currentTime = System.currentTimeMillis()
        
        // V√©rifier cooldown
        if (currentTime - lastRecognitionAt < RECOGNITION_COOLDOWN_MS) {
            Log.d("AutoBlind", "‚è≥ Recognition cooldown active, skipping...")
            return
        }
        
        // V√©rifier nombre de tentatives
        if (recognitionAttempts >= MAX_RECOGNITION_ATTEMPTS) {
            Log.w("AutoBlind", "‚ö†Ô∏è Max recognition attempts reached")
            detectionState = DetectionState.PERSON_IDENTIFIED
            voiceService.speak("Impossible de reconna√Ætre la personne.")
            hasAnnouncedPerson = true
            return
        }
        
        // Transition: ‚Üí RECOGNIZING
        detectionState = DetectionState.RECOGNIZING
        recognitionAttempts++
        lastRecognitionAt = currentTime
        
        Log.d("AutoBlind", "üîç Starting recognition (attempt $recognitionAttempts)...")
        
        // Appel API (silencieux)
        val result = faceRepository.recognizeFaces(bitmap, saveToHistory = true)
        
        result.onSuccess { response ->
            detectionCount++
            
            if (response.facesDetected == 0) {
                Log.w("AutoBlind", "‚ö†Ô∏è No faces in recognition result")
                
                // Retry si pas au max
                if (recognitionAttempts < MAX_RECOGNITION_ATTEMPTS) {
                    delay(300)
                    currentBitmap?.let { startRecognition(it) }
                } else {
                    resetToIdle()
                }
                return@onSuccess
            }
            
            val person = response.recognizedPersons.firstOrNull()
            
            if (person == null) {
                // Personne inconnue
                announceUnknownPerson()
                currentPersonId = "unknown"
                currentPersonName = "Inconnu"
                hasAnnouncedPerson = true
                detectionState = DetectionState.PERSON_IDENTIFIED
                return@onSuccess
            }
            
            // V√©rifier si c'est la m√™me personne qu'avant
            if (person.personId == currentPersonId && hasAnnouncedPerson) {
                Log.d("AutoBlind", "üîá Same person (${person.name}), staying silent...")
                detectionState = DetectionState.PERSON_IDENTIFIED
                return@onSuccess
            }
            
            // Nouvelle personne ou premi√®re annonce
            currentPersonId = person.personId
            currentPersonName = person.name
            hasAnnouncedPerson = true
            detectionState = DetectionState.PERSON_IDENTIFIED
            
            if (person.isRecognized && person.confidence >= MIN_CONFIDENCE) {
                announceRecognizedPerson(person)
            } else {
                announceUnknownPerson()
            }
        }.onFailure { error ->
            Log.e("AutoBlind", "‚ùå Recognition failed: ${error.message}")
            
            // Retry si pas au max
            if (recognitionAttempts < MAX_RECOGNITION_ATTEMPTS) {
                delay(500)
                currentBitmap?.let { startRecognition(it) }
            } else {
                detectionState = DetectionState.PERSON_IDENTIFIED
                voiceService.speak("Erreur de reconnaissance.")
                hasAnnouncedPerson = true
            }
        }
    }
    
    // ‚úÖ Traitement frame cam√©ra (MACHINE √Ä √âTATS)
    fun onCameraFrame(bitmap: Bitmap) {
        scope.launch {
            val currentTime = System.currentTimeMillis()
            
            // Limiter framerate d'analyse
            if (currentTime - lastAnalysisTime < FRAME_ANALYSIS_INTERVAL_MS) {
                return@launch
            }
            lastAnalysisTime = currentTime
            
            // D√©tection locale (rapide) avec position
            val (hasFace, facePosition) = detectFaceLocally(bitmap)
            
            when (detectionState) {
                DetectionState.IDLE -> {
                    if (hasFace) {
                        // Transition: IDLE ‚Üí FACE_DETECTED
                        faceFirstDetectedAt = currentTime
                        faceLastSeenAt = currentTime
                        consecutiveFramesWithFace = 1
                        consecutiveFramesWithoutFace = 0
                        lastFacePosition = facePosition  // ‚úÖ Sauvegarder position
                        detectionState = DetectionState.FACE_DETECTED
                        Log.d("AutoBlind", "üü¢ Face detected, waiting for stability...")
                    }
                }
                
                DetectionState.FACE_DETECTED -> {
                    if (hasFace) {
                        consecutiveFramesWithFace++
                        consecutiveFramesWithoutFace = 0  // ‚úÖ Reset counter
                        faceLastSeenAt = currentTime
                        lastFacePosition = facePosition  // ‚úÖ Mettre √† jour position
                        
                        // V√©rifier stabilit√©
                        val stableDuration = currentTime - faceFirstDetectedAt
                        if (stableDuration >= STABILITY_THRESHOLD_MS && !hasAnnouncedDetection) {
                            // Transition: FACE_DETECTED ‚Üí FACE_STABLE
                            detectionState = DetectionState.FACE_STABLE
                            hasAnnouncedDetection = true
                            voiceService.speak("Visage d√©tect√©.")
                            Log.d("AutoBlind", "üü¢ Face stable (${stableDuration}ms), announcing...")
                            
                            // Lancer reconnaissance imm√©diatement
                            startRecognition(bitmap)
                        }
                    } else {
                        consecutiveFramesWithoutFace++
                        
                        // ‚úÖ Tol√©rance am√©lior√©e: accepter quelques frames sans visage
                        if (consecutiveFramesWithoutFace > MAX_FRAMES_WITHOUT_FACE_BEFORE_RESET) {
                            Log.d("AutoBlind", "‚ö†Ô∏è Face disappeared before stability ($consecutiveFramesWithoutFace frames without face)")
                            resetToIdle()
                        }
                    }
                }
                
                DetectionState.FACE_STABLE -> {
                    // √âtat transitoire, g√©r√© par startRecognition
                    if (!hasFace) {
                        consecutiveFramesWithoutFace++
                        if (consecutiveFramesWithoutFace > 10) {
                            resetToIdle()
                        }
                    } else {
                        faceLastSeenAt = currentTime
                        lastFacePosition = facePosition
                    }
                }
                
                DetectionState.RECOGNIZING -> {
                    // Reconnaissance en cours, ne rien faire
                    if (!hasFace) {
                        consecutiveFramesWithoutFace++
                        
                        // Si visage dispara√Æt pendant reconnaissance
                        if (consecutiveFramesWithoutFace > 10) {
                            Log.w("AutoBlind", "‚ö†Ô∏è Face disappeared during recognition")
                            resetToIdle()
                        }
                    } else {
                        faceLastSeenAt = currentTime
                        consecutiveFramesWithoutFace = 0
                        lastFacePosition = facePosition
                    }
                }
                
                DetectionState.PERSON_IDENTIFIED -> {
                    // Annonce d√©j√† faite, en cooldown
                    if (hasFace) {
                        faceLastSeenAt = currentTime
                        consecutiveFramesWithoutFace = 0
                        
                        // ‚úÖ NOUVEAU: D√©tecter si le visage a chang√© de position (nouvelle personne)
                        if (hasFacePositionChanged(lastFacePosition, facePosition)) {
                            Log.d("AutoBlind", "üîÑ Face position changed significantly - new person detected!")
                            // Reset et red√©marrer la d√©tection
                            resetToIdle()
                            // Imm√©diatement passer √† FACE_DETECTED
                            faceFirstDetectedAt = currentTime
                            faceLastSeenAt = currentTime
                            consecutiveFramesWithFace = 1
                            lastFacePosition = facePosition
                            detectionState = DetectionState.FACE_DETECTED
                        } else {
                            // M√™me personne, mettre √† jour position
                            lastFacePosition = facePosition
                        }
                    } else {
                        consecutiveFramesWithoutFace++
                        
                        // Si personne dispara√Æt ‚â• 5s
                        val timeSinceLastSeen = currentTime - faceLastSeenAt
                        if (timeSinceLastSeen >= DISAPPEAR_THRESHOLD_MS) {
                            Log.d("AutoBlind", "üîÑ Person left (${timeSinceLastSeen}ms without face), resetting...")
                            resetToIdle()
                        }
                    }
                }
            }
        }
    }
    
    // ‚úÖ Permissions
    val permissionsState = rememberMultiplePermissionsState(
        permissions = listOf(
            Manifest.permission.RECORD_AUDIO,
            Manifest.permission.CAMERA
        )
    )

    // ‚úÖ G√©rer les commandes vocales
    LaunchedEffect(recognizedText) {
        if (recognizedText.isNotEmpty()) {
            val lower = recognizedText.lowercase()

            when {
                lower.contains("quitter") || lower.contains("sortir") ||
                        lower.contains("exit") || lower.contains("quit") -> {
                    showExitDialog = true
                }
                else -> {
                    // Envoyer au serveur
                    wsClient.sendVoiceCommand(recognizedText)
                }
            }
        }
    }

    // ‚úÖ G√©rer les r√©ponses du serveur
    LaunchedEffect(voiceResponse) {
        voiceResponse?.let { response ->
            response.speakText?.let { voiceService.speak(it) }

            when (response.action) {
                "navigate-back" -> {
                    delay(500)
                    navController.popBackStack()
                }
            }
        }
    }

    // ‚úÖ Initialisation (only connect if needed)
    LaunchedEffect(permissionsState.allPermissionsGranted) {
        if (permissionsState.allPermissionsGranted) {
            if (wsClient.shouldReconnect()) {
                delay(500)
                wsClient.connect()
            } else {
                Log.d("AutoBlind", "‚úÖ Socket already connected, reusing")
            }
            delay(1000)
            voiceService.startListening()
            isListening = true
            delay(1000)
            isCameraActive = true
            voiceService.speak("Mode automatique activ√©. Cam√©ra et micro en marche.")
        }
    }

    // ‚úÖ Cleanup (only local resources, not WebSocket)
    DisposableEffect(Unit) {
        onDispose {
            isCameraActive = false
            voiceService.cleanup()
            // Don't disconnect WebSocket - let MainActivity handle it
        }
    }

    Box(modifier = Modifier.fillMaxSize()) {
        // ‚úÖ Cam√©ra avec d√©tection
        if (isCameraActive) {
            CameraPreview(
                modifier = Modifier.fillMaxSize(),
                onFrameCaptured = { bitmap ->
                    currentBitmap = bitmap
                    onCameraFrame(bitmap)  // ‚úÖ Machine √† √©tats
                }
            )

        } else {
            Box(
                modifier = Modifier
                    .fillMaxSize()
                    .background(Color(0xFF212121)),
                contentAlignment = Alignment.Center
            ) {
                Text(
                    text = "Cam√©ra d√©sactiv√©e",
                    color = Color.White,
                    fontSize = 20.sp
                )
            }
        }

        // ‚úÖ Overlay d'informations
        Column(
            modifier = Modifier
                .fillMaxSize()
                .padding(20.dp)
        ) {
            // En-t√™te
            Card(
                modifier = Modifier.fillMaxWidth(),
                colors = CardDefaults.cardColors(
                    containerColor = Color.Black.copy(alpha = 0.7f)
                ),
                shape = RoundedCornerShape(16.dp)
            ) {
                Column(
                    modifier = Modifier.padding(16.dp),
                    verticalArrangement = Arrangement.spacedBy(8.dp)
                ) {
                    Row(
                        modifier = Modifier.fillMaxWidth(),
                        horizontalArrangement = Arrangement.SpaceBetween,
                        verticalAlignment = Alignment.CenterVertically
                    ) {
                        Text(
                            text = "Mode Auto",
                            fontSize = 20.sp,
                            fontWeight = FontWeight.Bold,
                            color = Color.White
                        )

                        Row(
                            horizontalArrangement = Arrangement.spacedBy(8.dp),
                            verticalAlignment = Alignment.CenterVertically
                        ) {
                            // √âtat WebSocket
                            Box(
                                modifier = Modifier
                                    .size(12.dp)
                                    .background(
                                        color = when (connectionState) {
                                            ConnectionState.CONNECTED -> Color(0xFF4CAF50)
                                            ConnectionState.CONNECTING -> Color(0xFFFFC107)
                                            else -> Color(0xFFF44336)
                                        },
                                        shape = CircleShape
                                    )
                            )

                            // √âtat micro
                            if (isListening) {
                                Icon(
                                    imageVector = Icons.Default.Mic,
                                    contentDescription = "Listening",
                                    tint = Color(0xFF4CAF50),
                                    modifier = Modifier.size(20.dp)
                                )
                            }
                        }
                    }

                    // Indicateur de d√©tection (bas√© sur machine √† √©tats)
                    Row(
                        verticalAlignment = Alignment.CenterVertically,
                        horizontalArrangement = Arrangement.spacedBy(8.dp)
                    ) {
                        val (icon, text, color) = when (detectionState) {
                            DetectionState.IDLE -> Triple(Icons.Default.Search, "Recherche en cours...", Color.White)
                            DetectionState.FACE_DETECTED -> Triple(Icons.Default.Face, "Visage en cours de stabilisation...", Color(0xFFFFC107))
                            DetectionState.FACE_STABLE -> Triple(Icons.Default.Face, "Visage stable", Color(0xFF4CAF50))
                            DetectionState.RECOGNIZING -> Triple(Icons.Default.Face, "Reconnaissance en cours...", Color(0xFF2196F3))
                            DetectionState.PERSON_IDENTIFIED -> {
                                val personName = currentPersonName ?: "Personne"
                                Triple(Icons.Default.Face, personName, Color(0xFF4CAF50))
                            }
                        }
                        
                        Icon(
                            imageVector = icon,
                            contentDescription = null,
                            tint = color,
                            modifier = Modifier.size(20.dp)
                        )
                        Text(
                            text = text,
                            color = Color.White,
                            fontSize = 14.sp
                        )
                    }

                    Text(
                        text = "D√©tections: $detectionCount",
                        color = Color.White.copy(alpha = 0.7f),
                        fontSize = 12.sp
                    )
                }
            }

            Spacer(modifier = Modifier.weight(1f))

            // ‚úÖ Commandes vocales
            if (recognizedText.isNotEmpty()) {
                Card(
                    modifier = Modifier.fillMaxWidth(),
                    colors = CardDefaults.cardColors(
                        containerColor = Color.Black.copy(alpha = 0.7f)
                    ),
                    shape = RoundedCornerShape(16.dp)
                ) {
                    Column(modifier = Modifier.padding(12.dp)) {
                        Text(
                            text = "Commande:",
                            fontSize = 12.sp,
                            fontWeight = FontWeight.Bold,
                            color = Color(0xFF4CAF50)
                        )
                        Text(
                            text = recognizedText,
                            fontSize = 14.sp,
                            color = Color.White
                        )
                    }
                }
            }

            Spacer(modifier = Modifier.height(16.dp))

            // ‚úÖ Boutons de contr√¥le
            Row(
                modifier = Modifier.fillMaxWidth(),
                horizontalArrangement = Arrangement.SpaceEvenly
            ) {
                // Toggle micro
                FloatingActionButton(
                    onClick = {
                        if (isListening) {
                            voiceService.stopListening()
                            isListening = false
                            voiceService.speak("Micro d√©sactiv√©")
                        } else {
                            voiceService.startListening()
                            isListening = true
                            voiceService.speak("Micro activ√©")
                        }
                    },
                    containerColor = if (isListening) Color(0xFF4CAF50) else Color.White
                ) {
                    Icon(
                        imageVector = if (isListening) Icons.Default.Mic else Icons.Default.MicOff,
                        contentDescription = "Toggle Mic",
                        tint = if (isListening) Color.White else Color.Black
                    )
                }

                // Toggle cam√©ra
                FloatingActionButton(
                    onClick = {
                        if (isCameraActive) {
                            isCameraActive = false
                            voiceService.speak("Cam√©ra d√©sactiv√©e")
                        } else {
                            isCameraActive = true
                            voiceService.speak("Cam√©ra activ√©e")
                        }
                    },
                    containerColor = if (isCameraActive) Color(0xFF4CAF50) else Color.White
                ) {
                    Icon(
                        imageVector = if (isCameraActive) Icons.Default.Videocam else Icons.Default.VideocamOff,
                        contentDescription = "Toggle Camera",
                        tint = if (isCameraActive) Color.White else Color.Black
                    )
                }

                // Aide
                FloatingActionButton(
                    onClick = {
                        wsClient.requestHelp()
                    },
                    containerColor = Color.White
                ) {
                    Icon(
                        imageVector = Icons.Default.Help,
                        contentDescription = "Help",
                        tint = Color.Black
                    )
                }

                // Quitter
                FloatingActionButton(
                    onClick = {
                        showExitDialog = true
                    },
                    containerColor = Color(0xFFF44336)
                ) {
                    Icon(
                        imageVector = Icons.Default.ExitToApp,
                        contentDescription = "Exit",
                        tint = Color.White
                    )
                }
            }
        }
    }

    // ‚úÖ Dialog de permissions
    if (!permissionsState.allPermissionsGranted) {
        AlertDialog(
            onDismissRequest = { },
            title = { Text("Permissions requises") },
            text = {
                Column {
                    Text("Cette fonctionnalit√© n√©cessite :")
                    Spacer(modifier = Modifier.height(8.dp))
                    Text("‚Ä¢ Microphone : commandes vocales")
                    Text("‚Ä¢ Cam√©ra : reconnaissance faciale")
                }
            },
            confirmButton = {
                TextButton(
                    onClick = {
                        permissionsState.launchMultiplePermissionRequest()
                    }
                ) {
                    Text("Autoriser")
                }
            }
        )
    }

    // ‚úÖ Dialog de sortie
    if (showExitDialog) {
        AlertDialog(
            onDismissRequest = { showExitDialog = false },
            title = { Text("Quitter le mode automatique ?") },
            text = { Text("La cam√©ra et le micro seront d√©sactiv√©s.") },
            confirmButton = {
                TextButton(
                    onClick = {
                        scope.launch {
                            isCameraActive = false
                            voiceService.cleanup()
                            // Don't disconnect WebSocket - let MainActivity handle it
                            voiceService.speak("Mode automatique d√©sactiv√©")
                            delay(1000)
                            navController.popBackStack()
                        }
                    }
                ) {
                    Text("Oui, quitter")
                }
            },
            dismissButton = {
                TextButton(onClick = { showExitDialog = false }) {
                    Text("Annuler")
                }
            }
        )
    }
}


@Composable
fun CameraPreview(
    modifier: Modifier = Modifier,
    onFrameCaptured: (Bitmap) -> Unit
) {
    val context = LocalContext.current
    val lifecycleOwner = LocalLifecycleOwner.current

    val previewView = remember { PreviewView(context) }

    AndroidView(
        factory = { previewView },
        modifier = modifier
    )

    LaunchedEffect(Unit) {
        val cameraProviderFuture = ProcessCameraProvider.getInstance(context)
        val cameraProvider = cameraProviderFuture.get()

        val preview = Preview.Builder().build().also {
            it.setSurfaceProvider(previewView.surfaceProvider)
        }

        val imageAnalyzer = ImageAnalysis.Builder()
            .setBackpressureStrategy(ImageAnalysis.STRATEGY_KEEP_ONLY_LATEST)
            .build()
            .also { analyzer ->
                analyzer.setAnalyzer(ContextCompat.getMainExecutor(context)) { imageProxy ->
                    // Convertir ImageProxy en Bitmap
                    val bitmap = imageProxy.toBitmap()
                    onFrameCaptured(bitmap)
                    imageProxy.close()
                }
            }

        val cameraSelector = CameraSelector.DEFAULT_BACK_CAMERA

        cameraProvider.unbindAll()
        cameraProvider.bindToLifecycle(lifecycleOwner, cameraSelector, preview, imageAnalyzer)
    }
}

// Extension helper
fun ImageProxy.toBitmap(): Bitmap {
    val yBuffer = planes[0].buffer
    val uBuffer = planes[1].buffer
    val vBuffer = planes[2].buffer

    val ySize = yBuffer.remaining()
    val uSize = uBuffer.remaining()
    val vSize = vBuffer.remaining()

    val nv21 = ByteArray(ySize + uSize + vSize)
    yBuffer.get(nv21, 0, ySize)
    vBuffer.get(nv21, ySize, vSize)
    uBuffer.get(nv21, ySize + vSize, uSize)

    val yuvImage = YuvImage(nv21, ImageFormat.NV21, width, height, null)
    val out = ByteArrayOutputStream()
    yuvImage.compressToJpeg(Rect(0, 0, width, height), 100, out)
    val imageBytes = out.toByteArray()
    return BitmapFactory.decodeByteArray(imageBytes, 0, imageBytes.size)
}
